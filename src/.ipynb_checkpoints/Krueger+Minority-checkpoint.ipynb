{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Study 1c: Study on 50 U.S. states. state - name of the state, city - name of the city asked\n",
    "# Every state is asked with it's true capital\n",
    "study_1c = pd.read_csv('~/DATA_1030/Final_Project/crowd_wisdom_data/study1c.csv')\n",
    "# Study 2: Trivia. qname - the topic of the trivia question (39 participants, 80 unique qnames)\n",
    "study_2 = pd.read_csv('~/DATA_1030/Final_Project/crowd_wisdom_data/study2.csv')\n",
    "# Study 3: Dermatologists diagnosing lesions as malignant or benign\n",
    "study_3 = pd.read_csv('~/DATA_1030/Final_Project/crowd_wisdom_data/study3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group the datasets based on questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating REAL minority values / cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_1c, for 33 participants\n",
    "grouped_1c = study_1c.groupby('state').sum()\n",
    "grouped_1c[\"minority_response\"] = 0\n",
    "count = study_1c.groupby('state').count()\n",
    "for ind, row in grouped_1c.iterrows():\n",
    "    size = count.loc[ind][0]\n",
    "    if grouped_1c.loc[ind][\"own\"] > size//2:\n",
    "        study_1c.loc[study_1c[\"state\"]==ind, \"minority_response\"] = 0\n",
    "        study_1c.loc[study_1c[\"state\"]==ind, \"minority_number\"] = size-grouped_1c.loc[ind][\"own\"]\n",
    "    else:\n",
    "        study_1c.loc[study_1c[\"state\"]==ind, \"minority_response\"] = 1\n",
    "        study_1c.loc[study_1c[\"state\"]==ind, \"minority_number\"] = grouped_1c.loc[ind][\"own\"]\n",
    "\n",
    "\n",
    "# study_2, for 39 participants\n",
    "grouped_2 = study_2.groupby('qtext').sum()\n",
    "grouped_2[\"minority_response\"] = 0\n",
    "count = study_2.groupby('qtext').count()\n",
    "for ind, row in grouped_2.iterrows():\n",
    "    size = count.loc[ind][0]\n",
    "    if grouped_2.loc[ind][\"own\"] > size//2:\n",
    "        study_2.loc[study_2[\"qtext\"]==ind, \"minority_response\"] = 0\n",
    "        study_2.loc[study_2[\"qtext\"]==ind, \"minority_number\"] = size-grouped_2.loc[ind][\"own\"]\n",
    "    else:\n",
    "        study_2.loc[study_2[\"qtext\"]==ind, \"minority_response\"] = 1\n",
    "        study_2.loc[study_2[\"qtext\"]==ind, \"minority_number\"] = grouped_2.loc[ind][\"own\"]\n",
    "        \n",
    "# study_3, for 25 participants\n",
    "grouped_3 = study_3.groupby('image').sum()\n",
    "grouped_3[\"minority_response\"] = 0\n",
    "count = study_3.groupby('image').count()\n",
    "for ind, row in grouped_3.iterrows():\n",
    "    size = count.loc[ind][0]\n",
    "    if grouped_3.loc[ind][\"own\"] > size//2:\n",
    "        study_3.loc[study_3[\"image\"]==ind, \"minority_response\"] = 0\n",
    "        study_3.loc[study_3[\"image\"]==ind, \"minority_number\"] = size-grouped_3.loc[ind][\"own\"]\n",
    "    else:\n",
    "        study_3.loc[study_3[\"image\"]==ind, \"minority_response\"] = 1\n",
    "        study_3.loc[study_3[\"image\"]==ind, \"minority_number\"] = grouped_3.loc[ind][\"own\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "for index, row in study_1c.iterrows():\n",
    "    avg.append(grouped_1c.loc[row[\"state\"]][\"own\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "for index, row in study_2.iterrows():\n",
    "    avg.append(grouped_2.loc[row[\"qtext\"]][\"own\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = []\n",
    "for index, row in study_3.iterrows():\n",
    "    avg.append(grouped_3.loc[row[\"image\"]][\"own\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minority classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_1c[\"in_minority\"] = np.where(study_1c['own'] == study_1c['minority_response'], 'yes', 'no') #so that 0,0 doesn't count as an expert\n",
    "study_1c[\"true_guess\"] = np.where(study_1c['own'] == study_1c['actual'], 'yes', 'no ')\n",
    "study_1c[\"expert\"] = np.where(study_1c['in_minority'] == study_1c['true_guess'], 'yes', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_2[\"in_minority\"] = np.where(study_2['own'] == study_2['minority_response'], 'yes', 'no') #so that 0,0 doesn't count as an expert\n",
    "study_2[\"true_guess\"] = np.where(study_2['own'] == study_2['actual'], 'yes', 'no ')\n",
    "study_2[\"expert\"] = np.where(study_2['in_minority'] == study_2['true_guess'], 'yes', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_3[\"in_minority\"] = np.where(study_3['own'] == study_3['minority_response'], 'yes', 'no') #so that 0,0 doesn't count as an expert\n",
    "study_3[\"true_guess\"] = np.where(study_3['own'] == study_3['actual'], 'yes', 'no ')\n",
    "study_3[\"expert\"] = np.where(study_3['in_minority'] == study_3['true_guess'], 'yes', 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create self-consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_1c[\"self_consensus\"] = np.where(study_1c['own'] == 0, 1-study_1c['meta'], study_1c['meta'])\n",
    "study_2[\"self_consensus\"] = np.where(study_2['own'] == 0, 1-study_2['meta'], study_2['meta'])\n",
    "study_3[\"self_consensus\"] = np.where(study_3['own'] == 0, 1-study_3['meta'], study_3['meta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Self-Concensus difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_1c[\"c-sc\"] = study_1c[\"confidence\"]-study_1c[\"self_consensus\"]\n",
    "study_2[\"c-sc\"] = study_2[\"confidence\"]-study_2[\"self_consensus\"]\n",
    "study_3[\"c-sc\"] = study_3[\"confidence\"]-study_3[\"self_consensus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert to Non-Expert Ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert to Non-Expert Ratio study_1c: 0.17857142857142858\n"
     ]
    }
   ],
   "source": [
    "experts1 = study_1c[study_1c[\"expert\"] == 'yes']\n",
    "non_experts1 = study_1c[study_1c[\"expert\"] == 'no']\n",
    "print(\"Expert to Non-Expert Ratio study_1c:\", experts1.shape[0]/non_experts1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert to Non-Expert Ratio study_2: 0.18143939393939393\n"
     ]
    }
   ],
   "source": [
    "experts2 = study_2[study_2[\"expert\"] == 'yes']\n",
    "non_experts2 = study_2[study_2[\"expert\"] == 'no']\n",
    "print(\"Expert to Non-Expert Ratio study_2:\", experts2.shape[0]/non_experts2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert to Non-Expert Ratio study_3: 0.07913669064748201\n"
     ]
    }
   ],
   "source": [
    "experts3 = study_3[study_3[\"expert\"] == 'yes']\n",
    "non_experts3 = study_3[study_3[\"expert\"] == 'no']\n",
    "print(\"Expert to Non-Expert Ratio study_3:\", experts3.shape[0]/non_experts3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert to Non-Expert Ratio total: 0.15451197053406998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "experts = pd.concat([experts1, experts2, experts3])\n",
    "non_experts = pd.concat([non_experts1, non_experts2, non_experts3])\n",
    "print(\"Expert to Non-Expert Ratio total:\", experts.shape[0]/non_experts.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krueger Statictics calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krueger_statictic(dta, predicting, cut):\n",
    "    from scipy.stats import ttest_ind\n",
    "    from statistics import stdev \n",
    "    import statistics\n",
    "    \n",
    "    if predicting == False:\n",
    "        experts = dta[dta[\"expert\"] == 'yes']\n",
    "        non_experts = dta[dta[\"expert\"] == 'no']\n",
    "    else:\n",
    "#         import pdb; pdb.set_trace()\n",
    "        question_title = dta.index\n",
    "        dta.reset_index(inplace=True, drop=True)\n",
    "        experts = dta[dta[\"in_minority\"] == \"yes\"][:cut]\n",
    "        non_experts = dta.drop(experts.index)\n",
    "        experts[\"question\"] = question_title[0]\n",
    "        experts.index = experts[\"question\"]\n",
    "        experts.drop([\"question\"], axis=1)\n",
    "        \n",
    "        non_experts[\"question\"] = question_title[0]\n",
    "        non_experts.index = non_experts[\"question\"]\n",
    "        non_experts.drop([\"question\"], axis=1)\n",
    "            \n",
    "    ''' p values less than 0.05, 95% confidence interval\n",
    "    '''\n",
    "    if experts.shape[0] == 0:\n",
    "        average_std = stdev(non_experts[\"c-sc\"])\n",
    "        mean_non_experts_sc = non_experts[\"c-sc\"].mean()\n",
    "        krueger_test_statistic = (0-mean_non_experts_sc)/average_std\n",
    "        return krueger_test_statistic\n",
    "    if experts.shape[0] == 1:\n",
    "        average_std = stdev(dta[\"c-sc\"])\n",
    "        mean_non_experts_sc = non_experts[\"c-sc\"].mean()\n",
    "        mean_experts_sc = experts[\"c-sc\"].mean()\n",
    "        krueger_test_statistic = (mean_experts_sc-mean_non_experts_sc)/average_std\n",
    "        return krueger_test_statistic\n",
    "    if experts.shape[0] > 1:\n",
    "        if predicting:\n",
    "            try:\n",
    "                determine = stdev(experts[\"c-sc\"])/stdev(non_experts[\"c-sc\"]) < 2 or stdev(experts[\"c-sc\"])/stdev(non_experts[\"c-sc\"]) > 0.5\n",
    "            except (ZeroDivisionError, statistics.StatisticsError):\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                determine = False\n",
    "            if determine:\n",
    "                average_std = (stdev(experts[\"c-sc\"])+stdev(non_experts[\"c-sc\"]))/2\n",
    "                mean_experts_sc = experts[\"c-sc\"].mean()\n",
    "                mean_non_experts_sc = non_experts[\"c-sc\"].mean()\n",
    "                krueger_test_statistic = (mean_experts_sc-mean_non_experts_sc)/average_std\n",
    "                return krueger_test_statistic\n",
    "            else:\n",
    "                return \"Sd more than double away\"\n",
    "        else:\n",
    "            t_test = ttest_ind(experts[\"c-sc\"], non_experts[\"c-sc\"])\n",
    "            t_test_p_value = t_test[1]\n",
    "            if t_test_p_value < 0.05:\n",
    "                try:\n",
    "                    determine = stdev(experts[\"c-sc\"])/stdev(non_experts[\"c-sc\"]) < 2 or stdev(experts[\"c-sc\"])/stdev(non_experts[\"c-sc\"]) > 0.5\n",
    "                except ZeroDivisionError:\n",
    "                    determine = False\n",
    "                if determine:\n",
    "                    average_std = (stdev(experts[\"c-sc\"])+stdev(non_experts[\"c-sc\"]))/2\n",
    "                    mean_experts_sc = experts[\"c-sc\"].mean()\n",
    "                    mean_non_experts_sc = non_experts[\"c-sc\"].mean()\n",
    "                    krueger_test_statistic = (mean_experts_sc-mean_non_experts_sc)/average_std\n",
    "                    return krueger_test_statistic\n",
    "                else:\n",
    "                    return \"Sd more than double away\"\n",
    "            else:\n",
    "                return f\"t-test not significant: {t_test_p_value}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Questions in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = study_1c[\"state\"].unique()\n",
    "study_1c = study_1c.sort_values(['state', 'c-sc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = study_2[\"qtext\"].unique()\n",
    "study_2 = study_2.sort_values(['qtext', 'c-sc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = study_3[\"image\"].unique()\n",
    "study_3 = study_3.sort_values(['image', 'c-sc'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating K-Stats for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from statistics import stdev\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_1c = study_1c.drop(['expt city', 'subject', 'own', 'meta', 'confidence', 'actual',\n",
    "               'minority_response', 'true_guess', 'self_consensus'], axis=1)\n",
    "\n",
    "study_2 = study_2.drop(['subject', 'qname', 'own', 'meta', 'actual', 'confidence',\n",
    "                        'minority_response','true_guess','self_consensus'], axis=1)\n",
    "\n",
    "study_3 = study_3.drop(['subject', 'own', 'actual', 'meta', 'confidence',\n",
    "                        'minority_response', 'true_guess', 'self_consensus'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_1c.columns = ['question', 'minority_number', 'in_minority','expert','c-sc']\n",
    "study_2.columns = ['question', 'minority_number', 'in_minority','expert','c-sc']\n",
    "study_3.columns = ['question', 'minority_number', 'in_minority','expert','c-sc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.concat([study_1c, study_2, study_3])\n",
    "df_questions = df_questions.reset_index()\n",
    "df_questions = df_questions.drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark expert empty datas\n",
    "df_questions[\"has_at_least_one_expert\"] = 0\n",
    "df_questions[\"real_expert_number\"] = 0\n",
    "for q in df_questions[\"question\"].unique():\n",
    "    expert_number = df_questions[df_questions[\"question\"] == q][\"expert\"].str.count(\"yes\").sum()\n",
    "    if expert_number > 0:\n",
    "        df_questions.loc[df_questions[\"question\"] == q, \"has_at_least_one_expert\"] = 1\n",
    "    df_questions.loc[df_questions[\"question\"] == q, \"real_expert_number\"] = expert_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_expert_df = df_questions.groupby([\"question\"]).mean().drop(['minority_number', 'c-sc',\n",
    "       'has_at_least_one_expert'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(real_values, predicted_values):\n",
    "    return ((real_values - predicted_values) ** 2).mean() ** .5\n",
    "'''\n",
    "Random refers to which rows will be selected as expert when the selections is being done.\n",
    "Default, non random value is to sort the dataframe in ascending order on c-sc, and then\n",
    "select k amount of rows based on the cutoff percentage. \n",
    "'''\n",
    "\n",
    "def predict_expert_numbers(df_question, random, X_train):\n",
    "    df_question[\"best_krueger_stat_for_the_prediction\"] = 0\n",
    "    df_question[\"diff_krueger_stat_real_prediction\"] = 0\n",
    "    df_question[\"predicted_expert_number\"] = 0\n",
    "    df_question[\"predicted_expert_number_PERCENTAGE\"] = 0\n",
    "    best_score_list = []\n",
    "    questions_list = df_question.index.unique()\n",
    "    \n",
    "    for index in range(len(questions_list)):\n",
    "        q_id = questions_list[index]\n",
    "        df = df_question.loc[q_id]\n",
    "        \n",
    "        score_list = []\n",
    "        \n",
    "        \n",
    "        if random == False:\n",
    "            df = df.sort_values(['c-sc'], ascending=False)\n",
    "            cutoffs = list(range(1, 101))\n",
    "            cutoffs_l = [round(x/100*(int(df[\"minority_number\"].mean()))) for x in cutoffs]\n",
    "            cutoff_dict= dict(zip(cutoffs, cutoffs_l))\n",
    "\n",
    "            for perc, cut in cutoff_dict.items():\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                k_stat = krueger_statictic(df, True, cut)\n",
    "                percentage_of_total_population = cut/len(df)\n",
    "                best_d = best_distance(k_stat, X_train)\n",
    "                threshold_used = best_d[0][1]\n",
    "                score_list.append([q_id,k_stat, best_d[0][0], threshold_used,\n",
    "                                   cut,percentage_of_total_population])\n",
    "        else:\n",
    "            for x in range(0,10):\n",
    "#                 df = df.sample(frac=1, random_state=int(x**3)).reset_index(drop=True)\n",
    "                df = df.sample(frac=1, random_state=int(x**3))\n",
    "                cutoffs = list(range(1, 101))\n",
    "    \n",
    "                try:\n",
    "                    cutoffs_l = [round(x/100*(int(df[\"minority_number\"].mean()))) for x in cutoffs]\n",
    "                except ValueError:\n",
    "                    import pdb; pdb.set_trace()\n",
    "\n",
    "                cutoffs_l = [round(x/100*(int(df[\"minority_number\"].mean()))) for x in cutoffs]\n",
    "                cutoff_dict= dict(zip(cutoffs, cutoffs_l))\n",
    "\n",
    "                for perc, cut in cutoff_dict.items():\n",
    "                    k_stat = krueger_statictic(df, True, cut)\n",
    "                    percentage_of_total_population = cut/len(df)\n",
    "                    best_d = best_distance(k_stat, X_train)\n",
    "                    threshold_used = best_d[0][1]\n",
    "                    score_list.append([q_id,k_stat, best_d[0][0], threshold_used,\n",
    "                                       cut,percentage_of_total_population])\n",
    "                                        \n",
    "        score_list.sort(key=lambda tup: tup[2],reverse=False)\n",
    "        best_score_list.append(score_list[0])\n",
    "        \n",
    "    '''\n",
    "    Checks for the datasets which have a minority grou however, the minority group is wrong. \n",
    "    This means there are 0 experts in the dataset as per our definition of an expert (actual==own, in_minority=1).\n",
    "    '''\n",
    "        \n",
    "    for i in range(len(best_score_list)):\n",
    "        q = best_score_list[i][0]\n",
    "        df_question.loc[q, \"best_krueger_stat_for_the_prediction\"] = best_score_list[i][3]\n",
    "        df_question.loc[q, \"diff_krueger_stat_real_prediction\"] = best_score_list[i][2]\n",
    "        if best_score_list[i][3] < 0: #if the threshold used is closer to the threshold of 0s\n",
    "            expert_number = 0\n",
    "            expert_perc = 0\n",
    "        else:\n",
    "            expert_number = best_score_list[i][4]\n",
    "            expert_perc = best_score_list[i][5]\n",
    "        df_question.loc[q, \"predicted_expert_number\"] = expert_number\n",
    "        df_question.loc[q, \"predicted_expert_number_PERCENTAGE\"] = expert_perc\n",
    "        \n",
    "def best_distance(k_stat, X_train):\n",
    "    thresholds = X_train[\"k_stat_thresh\"].unique()\n",
    "    best_distance = []\n",
    "    comp1 = abs(float(thresholds[0]-k_stat))\n",
    "    comp2 = abs(float(thresholds[1]-k_stat))\n",
    "    if comp1 < comp2:\n",
    "        best_distance.append([comp1,thresholds[0]])\n",
    "    else:\n",
    "        best_distance.append([comp2,thresholds[1]])\n",
    "    return best_distance\n",
    "\n",
    "def create_test_train(df_questions):\n",
    "    df_questions = df_questions.set_index(\"question\")\n",
    "    size = 210\n",
    "    \n",
    "    qs = df_questions.index.unique()\n",
    "    \n",
    "    dt1 = np.random.choice(qs, size//5, replace=False)\n",
    "    dt1 = df_questions.loc[dt1]\n",
    "    df_questions = df_questions.drop(dt1.index)\n",
    "\n",
    "    qs = df_questions.index.unique()\n",
    "    dt2 = np.random.choice(qs, size//5, replace=False)\n",
    "    dt2 = df_questions.loc[dt2]\n",
    "    df_questions = df_questions.drop(dt2.index)\n",
    "\n",
    "    qs = df_questions.index.unique()\n",
    "    dt3 = np.random.choice(qs, size//5, replace=False)\n",
    "    dt3 = df_questions.loc[dt3]\n",
    "    df_questions = df_questions.drop(dt3.index)\n",
    "\n",
    "    qs = df_questions.index.unique()\n",
    "    dt4 = np.random.choice(qs, size//5, replace=False)\n",
    "    dt4 = df_questions.loc[dt4]\n",
    "    df_questions = df_questions.drop(dt4.index)\n",
    "\n",
    "    qs = df_questions.index.unique()\n",
    "    dt5 = np.random.choice(qs, len(qs), replace=False)\n",
    "    dt5 = df_questions.loc[dt5]\n",
    "    df_questions = df_questions.drop(dt5.index) \n",
    "    \n",
    "    datasets = [dt1, dt2, dt3, dt4, dt5]\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in df_questions[\"question\"].unique():\n",
    "    df_questions.loc[df_questions[\"question\"] == q, \"minority_percntge\"] = (df_questions.loc[df_questions[\"question\"] == q][\"minority_number\"].mean()/len(df_questions.loc[df_questions[\"question\"] == q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 started.\n",
      "Krueger train works.\n",
      "Round 0 prediction ended.\n",
      "Round 1 started.\n",
      "Krueger train works.\n",
      "Round 1 prediction ended.\n",
      "Round 2 started.\n",
      "Krueger train works.\n",
      "Round 2 prediction ended.\n",
      "Round 3 started.\n",
      "Krueger train works.\n",
      "Round 3 prediction ended.\n",
      "Round 4 started.\n",
      "Krueger train works.\n",
      "Round 4 prediction ended.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create test and train dataset\n",
    "Get an average Krueger score (of the 5) \n",
    "    AND Run predictions 5 times, see which is more accurate\n",
    "'''\n",
    "no_minority_situations = df_questions[df_questions[\"minority_number\"] == 0.0]\n",
    "df_questions = df_questions.drop(no_minority_situations.index)\n",
    "\n",
    "random = [True,False]\n",
    "trainKStat_expert_list = []\n",
    "trainKStat_noExpert_list = []\n",
    "\n",
    "data_sets = create_test_train(df_questions)\n",
    "\n",
    "for i in range(len(data_sets)):\n",
    "    print(f\"Round {i} started.\")\n",
    "    test_data = data_sets[i]\n",
    "    X_test = test_data.drop([\"real_expert_number\"], axis=1)\n",
    "    y_test = test_data[\"real_expert_number\"]\n",
    "    \n",
    "    train_data = []\n",
    "    for index in range(len(data_sets)):\n",
    "        if index != i:\n",
    "            train_data.append(data_sets[index])\n",
    "\n",
    "    X_train = pd.concat(train_data)    \n",
    "    X_train[\"k_stat_thresh\"] = 0\n",
    "    \n",
    "    trainKStat_expert = krueger_statictic(X_train[X_train[\"has_at_least_one_expert\"] == 1], False, 0)\n",
    "    print(\"Krueger train works.\")\n",
    "    trainKStat_noExpert = krueger_statictic(X_train[X_train[\"has_at_least_one_expert\"] == 0], False, 0)\n",
    "    trainKStat_expert_list.append(trainKStat_expert)\n",
    "    trainKStat_noExpert_list.append(trainKStat_noExpert)\n",
    "    \n",
    "    X_train.loc[X_train[\"has_at_least_one_expert\"] == 1, \"k_stat_thresh\"] = trainKStat_expert\n",
    "    X_train.loc[X_train[\"has_at_least_one_expert\"] == 0, \"k_stat_thresh\"] = trainKStat_noExpert\n",
    "\n",
    "    for rand in random:\n",
    "        predict_expert_numbers(X_test, rand, X_train)\n",
    "        if rand:\n",
    "            comparing_data = X_test.groupby('question').mean()\n",
    "            comparing_data = comparing_data.drop(['minority_number', \n",
    "                                              'c-sc','has_at_least_one_expert',\n",
    "                                              'best_krueger_stat_for_the_prediction',\n",
    "                                              'diff_krueger_stat_real_prediction',\n",
    "                                              'predicted_expert_number_PERCENTAGE'], axis=1)\n",
    "        else:\n",
    "            comparing_data[\"predict_random_F\"] = X_test.groupby('question').mean()[\"predicted_expert_number\"]\n",
    "    print(f\"Round {i} prediction ended.\")\n",
    "            \n",
    "    comparing_data[\"real_expert_number\"] = 0\n",
    "    comparing_data.update(real_expert_df)\n",
    "    comparing_data.columns = [\"minority_percentage\", \"real_expert_number\", \"P_expert_number_rT\", \"P_expert_number_rF\"]\n",
    "    no_minority_situations = no_minority_situations.groupby(\"question\").mean()\n",
    "    if i == 0:\n",
    "        final_data_sets = comparing_data\n",
    "    else:\n",
    "        final_data_sets = pd.concat([final_data_sets, comparing_data])\n",
    "        \n",
    "for ind, row in no_minority_situations.iterrows():\n",
    "    final_data_sets.loc[ind] = [0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_expert_mean = sum(trainKStat_expert_list)/5\n",
    "k_nonexpert_mean = sum(trainKStat_noExpert_list)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSE(final_data_sets[\"real_expert_number\"], final_data_sets[\"P_expert_number_rT\"])\n",
    "mse_rF = MSE(final_data_sets[\"real_expert_number\"], final_data_sets[\"P_expert_number_rF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_sets.to_csv(\"krueger+minority.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Do it with their means\n",
    "'''\n",
    "\n",
    "data_sets = create_test_train(df_questions)\n",
    "X_test = pd.concat(data_sets) \n",
    "X_train = pd.concat(data_sets) \n",
    "X_train[\"k_stat_thresh\"] = 0\n",
    "\n",
    "X_train.loc[X_train[\"has_at_least_one_expert\"] == 1, \"k_stat_thresh\"] = k_expert_mean\n",
    "X_train.loc[X_train[\"has_at_least_one_expert\"] == 0, \"k_stat_thresh\"] = k_nonexpert_mean\n",
    "\n",
    "for rand in random:\n",
    "    predict_expert_numbers(X_test, rand, X_train)\n",
    "    if rand:\n",
    "        comparing_data = X_test.groupby('question').mean()\n",
    "        comparing_data = comparing_data.drop(['minority_number', \n",
    "                                          'c-sc','has_at_least_one_expert',\n",
    "                                          'best_krueger_stat_for_the_prediction',\n",
    "                                          'diff_krueger_stat_real_prediction',\n",
    "                                          'predicted_expert_number_PERCENTAGE'], axis=1)\n",
    "    else:\n",
    "        comparing_data[\"predict_random_F\"] = X_test.groupby('question').mean()[\"predicted_expert_number\"]\n",
    "\n",
    "comparing_data[\"real_expert_number\"] = 0\n",
    "comparing_data.update(real_expert_df)\n",
    "comparing_data.columns = [\"real_expert_number\", \"minority_percentage\", \"P_expert_number_rT\", \"P_expert_number_rF\"]\n",
    "no_minority_situations = no_minority_situations.groupby(\"question\").mean()\n",
    "\n",
    "for ind, row in no_minority_situations.iterrows():\n",
    "    comparing_data.loc[ind] = [0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing_data.columns = [\"real_expert_number\", \"minority_percentage\", \"MEAN_P_expert_rT\", \"MEAN_P_expert_rF\"]\n",
    "comparing_data.sort_index(inplace=True)\n",
    "final_data_sets.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.233591137113807, 5.848890818738525, 3.0237157840738176, 5.333630944077313)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = MSE(final_data_sets[\"real_expert_number\"], final_data_sets[\"P_expert_number_rT\"])\n",
    "mse_rF = MSE(final_data_sets[\"real_expert_number\"], final_data_sets[\"P_expert_number_rF\"])\n",
    "mse_m = MSE(final_data_sets[\"real_expert_number\"], final_data_sets[\"MEAN_P_expert_rT\"])\n",
    "mse_rF_m = MSE(final_data_sets[\"real_expert_number\"], final_data_sets[\"MEAN_P_expert_rF\"])\n",
    "mse, mse_rF, mse_m, mse_rF_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_sets[\"MEAN_P_expert_rT\"] = comparing_data[\"MEAN_P_expert_rT\"]\n",
    "final_data_sets[\"MEAN_P_expert_rF\"] = comparing_data[\"MEAN_P_expert_rF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_sets.to_csv(\"krueger+minority.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
